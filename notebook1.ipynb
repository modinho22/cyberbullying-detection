{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 2293133,
          "sourceType": "datasetVersion",
          "datasetId": 1382245
        }
      ],
      "dockerImageVersionId": 30732,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "name": "notebook1",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/modinho22/cyberbullying-detection/blob/main/notebook1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'suspicious-communication-on-social-platforms:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F1382245%2F2293133%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240731%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240731T185208Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D1243d022fc4dbab8a8c369fdf6980d3bce2694be3f04f9d03e31b8944fc5f5446e49aa294d20ba4fc0d9b5d48445e6305197673c71ac21eb0e572138147f366ea72af45a89222d24b2849d565d5ca9c966849af58fc247f266972f5579b6d7b08d31831f1cf3092c5943caef4c1b6b3d0de560dc180307c8493eb726f5a9e3add7eca8583fcc2310b5b3f6c3d6dd5c15ed6702c60d61db1d0d828730047d2a4aff94128f6936f139db8783dab1cc169675d61f3986b6fe07280aec10f15fcdd1dc113bcfbb09e709dd64978c3d47e829bd072deeea4bad35e6b5ccdd35f2548b2c635118e84f8f09eee0c5a03bfb2d9e013d71087f3f3f21056d44864f2cde87'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "CSu7rdwBIrkX"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "#reading the csv file as a link to github\n",
        "url = '/kaggle/input/suspicious-communication-on-social-platforms/Suspicious Communication on Social Platforms.csv'\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "#looking at the first five rows using .head() function\n",
        "df.head()\n",
        "\n",
        "#importing natural language Toolkit - A tool to preprocesses/clean text\n",
        "import nltk"
      ],
      "metadata": {
        "id": "2f9886d7",
        "papermill": {
          "duration": 1.793475,
          "end_time": "2021-11-07T17:22:09.284101",
          "exception": false,
          "start_time": "2021-11-07T17:22:07.490626",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2024-07-16T05:40:38.120413Z",
          "iopub.execute_input": "2024-07-16T05:40:38.121626Z",
          "iopub.status.idle": "2024-07-16T05:40:38.167177Z",
          "shell.execute_reply.started": "2024-07-16T05:40:38.121576Z",
          "shell.execute_reply": "2024-07-16T05:40:38.16589Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Checking that the latest nltk version is installed in the users local computer\n",
        "#Warning: install all the libraries required for this project\n",
        "nltk.__version__"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.04875,
          "end_time": "2021-11-07T17:22:09.367481",
          "exception": false,
          "start_time": "2021-11-07T17:22:09.318731",
          "status": "completed"
        },
        "tags": [],
        "id": "8964d4be",
        "execution": {
          "iopub.status.busy": "2024-07-16T05:40:38.169744Z",
          "iopub.execute_input": "2024-07-16T05:40:38.170143Z",
          "iopub.status.idle": "2024-07-16T05:40:38.177701Z",
          "shell.execute_reply.started": "2024-07-16T05:40:38.170106Z",
          "shell.execute_reply": "2024-07-16T05:40:38.176466Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.034325,
          "end_time": "2021-11-07T17:22:09.436424",
          "exception": false,
          "start_time": "2021-11-07T17:22:09.402099",
          "status": "completed"
        },
        "tags": [],
        "id": "5062fd37"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "2544b154",
        "papermill": {
          "duration": 0.04167,
          "end_time": "2021-11-07T17:22:09.512211",
          "exception": false,
          "start_time": "2021-11-07T17:22:09.470541",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2024-07-16T05:40:38.179552Z",
          "iopub.execute_input": "2024-07-16T05:40:38.180109Z",
          "iopub.status.idle": "2024-07-16T05:40:38.191412Z",
          "shell.execute_reply.started": "2024-07-16T05:40:38.180056Z",
          "shell.execute_reply": "2024-07-16T05:40:38.189911Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#converting all missing values in tagging column into NaN\n",
        "#example: converting None,NotAvailable,ValueMissing,etc into Nan\n",
        "df=df[pd.to_numeric(df['tagging'], errors='coerce').notnull()]"
      ],
      "metadata": {
        "id": "1bf874bc",
        "papermill": {
          "duration": 0.050388,
          "end_time": "2021-11-07T17:22:09.597087",
          "exception": false,
          "start_time": "2021-11-07T17:22:09.546699",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2024-07-16T05:40:38.19317Z",
          "iopub.execute_input": "2024-07-16T05:40:38.194023Z",
          "iopub.status.idle": "2024-07-16T05:40:38.205303Z",
          "shell.execute_reply.started": "2024-07-16T05:40:38.19397Z",
          "shell.execute_reply": "2024-07-16T05:40:38.203975Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Droping all rows that contain Nan value\n",
        "df = df.dropna(axis=0)"
      ],
      "metadata": {
        "id": "81a39e59",
        "papermill": {
          "duration": 0.09345,
          "end_time": "2021-11-07T17:22:09.724976",
          "exception": false,
          "start_time": "2021-11-07T17:22:09.631526",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2024-07-16T05:40:38.208524Z",
          "iopub.execute_input": "2024-07-16T05:40:38.208959Z",
          "iopub.status.idle": "2024-07-16T05:40:38.221676Z",
          "shell.execute_reply.started": "2024-07-16T05:40:38.208922Z",
          "shell.execute_reply": "2024-07-16T05:40:38.220165Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Since the shape is the same there are no missing valus in our dataset\n",
        "df.shape"
      ],
      "metadata": {
        "id": "4df86968",
        "papermill": {
          "duration": 0.043539,
          "end_time": "2021-11-07T17:22:09.80399",
          "exception": false,
          "start_time": "2021-11-07T17:22:09.760451",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2024-07-16T05:40:38.223476Z",
          "iopub.execute_input": "2024-07-16T05:40:38.224609Z",
          "iopub.status.idle": "2024-07-16T05:40:38.233462Z",
          "shell.execute_reply.started": "2024-07-16T05:40:38.224565Z",
          "shell.execute_reply": "2024-07-16T05:40:38.232187Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "from nltk import pos_tag#pos_tag is a tool that tags the part of speech to the word(POS = Part of Speech)\n",
        "#example: tagging the word 'drinking' as verb\n",
        "\n",
        "#function for removing punctuations\n",
        "def tokenize_remove_punctuation(text):\n",
        "  clean_text = []         #creaating an empty list to store the cleaned text\n",
        "  text = text.split(\" \")  #spliting all words in a sentence separated by \" \" and storing them in a list named 'text'\n",
        "  for word in text:\n",
        "    word = list(word)  #spliting all words into alphabets\n",
        "    new_word = []      #creaating an empty list to store the new word after removing puntuations\n",
        "\n",
        "    # spliting the words into alphabets is used because it will convert words like 'reading?' into 'reading'\n",
        "    for c in word:\n",
        "      if c not in string.punctuation:     #string.puntuation is a list og all puntuation marks , example :@!$%&?, etc.\n",
        "        new_word.append(c)\n",
        "      word = \"\".join(new_word)  #joing the alphabets to create the word after removing all puntuations\n",
        "    clean_text.append(word)     #storing the word in the list named 'clean_text' to create the list of words in the sentence\n",
        "  return clean_text\n"
      ],
      "metadata": {
        "id": "29889a28",
        "papermill": {
          "duration": 0.04279,
          "end_time": "2021-11-07T17:22:09.882234",
          "exception": false,
          "start_time": "2021-11-07T17:22:09.839444",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2024-07-16T05:40:38.235054Z",
          "iopub.execute_input": "2024-07-16T05:40:38.235817Z",
          "iopub.status.idle": "2024-07-16T05:40:38.245619Z",
          "shell.execute_reply.started": "2024-07-16T05:40:38.235779Z",
          "shell.execute_reply": "2024-07-16T05:40:38.244347Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# using a sample sentence to see whether the funtion works well or not\n",
        "# NOTE: We have only created the functions till now. We haven't done anythong with our dataset till now.\n",
        "trial_text = tokenize_remove_punctuation(\"hello @anyone reading? wt is the name of am in that this  ??!@\")\n",
        "trial_text"
      ],
      "metadata": {
        "id": "98334ea7",
        "papermill": {
          "duration": 0.043803,
          "end_time": "2021-11-07T17:22:09.961364",
          "exception": false,
          "start_time": "2021-11-07T17:22:09.917561",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2024-07-16T05:40:38.247008Z",
          "iopub.execute_input": "2024-07-16T05:40:38.247438Z",
          "iopub.status.idle": "2024-07-16T05:40:38.266563Z",
          "shell.execute_reply.started": "2024-07-16T05:40:38.247399Z",
          "shell.execute_reply": "2024-07-16T05:40:38.265362Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "#downloads the list of stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "#'stopwords' is a list of words that have nearly no value in the sentence\n",
        "#example : 'I am a boy' is converted into 'boy'\n",
        "#here words like 'I', 'am', 'a' ;these words have very less comtribution to the sentence"
      ],
      "metadata": {
        "id": "e5026a1e",
        "papermill": {
          "duration": 0.330189,
          "end_time": "2021-11-07T17:22:10.327184",
          "exception": false,
          "start_time": "2021-11-07T17:22:09.996995",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2024-07-16T05:40:38.2684Z",
          "iopub.execute_input": "2024-07-16T05:40:38.268819Z",
          "iopub.status.idle": "2024-07-16T05:40:38.284549Z",
          "shell.execute_reply.started": "2024-07-16T05:40:38.268782Z",
          "shell.execute_reply": "2024-07-16T05:40:38.283214Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#storing all the stopwords in the list named 'stopwords'\n",
        "stopwords = nltk.corpus.stopwords.words('english')  #storing only english stopwords , there are stopwords for other language also such as chinese and french\n",
        "\n",
        "# Function to remove all the stopwords from the sentence\n",
        "def remove_stopwords(text):\n",
        "  clean_text = []\n",
        "  for word in text:\n",
        "    if word not in stopwords:\n",
        "      clean_text.append(word)\n",
        "  return clean_text"
      ],
      "metadata": {
        "id": "54e06c4d",
        "papermill": {
          "duration": 0.083615,
          "end_time": "2021-11-07T17:22:10.451165",
          "exception": false,
          "start_time": "2021-11-07T17:22:10.36755",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2024-07-16T05:40:38.286231Z",
          "iopub.execute_input": "2024-07-16T05:40:38.287358Z",
          "iopub.status.idle": "2024-07-16T05:40:38.295407Z",
          "shell.execute_reply.started": "2024-07-16T05:40:38.287306Z",
          "shell.execute_reply": "2024-07-16T05:40:38.294239Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# using a sample sentence to see whether the funtion works well or not\n",
        "remove_stopwords(trial_text)"
      ],
      "metadata": {
        "id": "cf3011a8",
        "papermill": {
          "duration": 0.071768,
          "end_time": "2021-11-07T17:22:10.588041",
          "exception": false,
          "start_time": "2021-11-07T17:22:10.516273",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2024-07-16T05:40:38.297045Z",
          "iopub.execute_input": "2024-07-16T05:40:38.297557Z",
          "iopub.status.idle": "2024-07-16T05:40:38.315789Z",
          "shell.execute_reply.started": "2024-07-16T05:40:38.297517Z",
          "shell.execute_reply": "2024-07-16T05:40:38.314276Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#tagging all the words according o their part of speech\n",
        "def pos_tagging(text):\n",
        "    try:\n",
        "        tagged = nltk.pos_tag(text)\n",
        "        return tagged\n",
        "    except Excepton as e:\n",
        "        print(e)"
      ],
      "metadata": {
        "id": "4da8ba60",
        "papermill": {
          "duration": 0.074932,
          "end_time": "2021-11-07T17:22:10.731464",
          "exception": false,
          "start_time": "2021-11-07T17:22:10.656532",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2024-07-16T05:40:38.317658Z",
          "iopub.execute_input": "2024-07-16T05:40:38.318069Z",
          "iopub.status.idle": "2024-07-16T05:40:38.327572Z",
          "shell.execute_reply.started": "2024-07-16T05:40:38.318034Z",
          "shell.execute_reply": "2024-07-16T05:40:38.32598Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import wordnet\n",
        "\n",
        "#wordnet is a tool that reads that reads the tagging and returns the part of speech\n",
        "def get_wordnet(pos_tag):\n",
        "  if pos_tag.startswith('J'):\n",
        "    return wordnet.ADJ\n",
        "  elif pos_tag.startswith('V'):\n",
        "    return wordnet.VERB\n",
        "  elif pos_tag.startswith('N'):\n",
        "    return wordnet.NOUN\n",
        "  elif pos_tag.startswith('R'):\n",
        "    return wordnet.ADV\n",
        "  else:\n",
        "    return wordnet.NOUN"
      ],
      "metadata": {
        "id": "8de298b7",
        "papermill": {
          "duration": 0.072566,
          "end_time": "2021-11-07T17:22:10.867858",
          "exception": false,
          "start_time": "2021-11-07T17:22:10.795292",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2024-07-16T05:40:38.329256Z",
          "iopub.execute_input": "2024-07-16T05:40:38.329752Z",
          "iopub.status.idle": "2024-07-16T05:40:38.341222Z",
          "shell.execute_reply.started": "2024-07-16T05:40:38.329713Z",
          "shell.execute_reply": "2024-07-16T05:40:38.339835Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "#WordLemmatizer is a tool that converts word into root word\n",
        "#Example: historical(word) is converted into history(root-word)\n",
        "\n",
        "#Now we will create a function that uses all the functions that we have created above\n",
        "\n",
        "def clean_text(text):\n",
        "  text = str(text)\n",
        "  #Converting text to lower-case\n",
        "  text = text.lower()\n",
        "  #tokenize and remove punctuations from the text\n",
        "  text = tokenize_remove_punctuation(text)\n",
        "  #remove words containing numericals\n",
        "  text = [word for word in text if not any(c.isdigit() for c in word)]\n",
        "  #remove stopwords\n",
        "  text = remove_stopwords(text)\n",
        "  #remove empty tokens\n",
        "  text = [ t for t in text if len(t) > 0]\n",
        "  #pos tagging\n",
        "  pos_tags = pos_tagging(text)\n",
        "  #Lemmatize text\n",
        "  text = [WordNetLemmatizer().lemmatize(t[0],get_wordnet(t[1])) for t in pos_tags]\n",
        "  #remove words with only one letter\n",
        "  text = [ t for t in text if len(t)>1]\n",
        "  #join all words\n",
        "  text = \" \".join(text)\n",
        "  return text"
      ],
      "metadata": {
        "id": "991de580",
        "papermill": {
          "duration": 0.059501,
          "end_time": "2021-11-07T17:22:10.987554",
          "exception": false,
          "start_time": "2021-11-07T17:22:10.928053",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2024-07-16T05:40:38.347101Z",
          "iopub.execute_input": "2024-07-16T05:40:38.347521Z",
          "iopub.status.idle": "2024-07-16T05:40:38.359162Z",
          "shell.execute_reply.started": "2024-07-16T05:40:38.347475Z",
          "shell.execute_reply": "2024-07-16T05:40:38.357775Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#don't know what is 'averaged_perceptron_tagger'\n",
        "#don't know why devansh downloaded it\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "#Downloading the wordnet tool\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "id": "2494487b",
        "papermill": {
          "duration": 0.101421,
          "end_time": "2021-11-07T17:22:11.125361",
          "exception": false,
          "start_time": "2021-11-07T17:22:11.02394",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2024-07-16T05:40:38.361315Z",
          "iopub.execute_input": "2024-07-16T05:40:38.361752Z",
          "iopub.status.idle": "2024-07-16T05:40:38.379364Z",
          "shell.execute_reply.started": "2024-07-16T05:40:38.361712Z",
          "shell.execute_reply": "2024-07-16T05:40:38.377854Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# using a sample sentence to see whether the funtion works well or not\n",
        "clean_text(\"What is y0ur name? THis is a cat!! 12?\")"
      ],
      "metadata": {
        "id": "e9bf3d70",
        "papermill": {
          "duration": 1.9575,
          "end_time": "2021-11-07T17:22:13.11962",
          "exception": false,
          "start_time": "2021-11-07T17:22:11.16212",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2024-07-16T05:40:38.381107Z",
          "iopub.execute_input": "2024-07-16T05:40:38.381602Z",
          "iopub.status.idle": "2024-07-16T05:40:38.395848Z",
          "shell.execute_reply.started": "2024-07-16T05:40:38.381553Z",
          "shell.execute_reply": "2024-07-16T05:40:38.394517Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['tagging'].value_counts(normalize=True).plot(kind='bar', title='Ratio of observations')\n",
        "#ploting number of '1' and '0' in the 'tagging' colomn of the dataset\n",
        "\n",
        "#As we can see the difference in number of '1' and '0' is not very large hence we call it as a balanced dataset\n",
        "#NOTE:It's very important to provide a balanced dataset for creating the model"
      ],
      "metadata": {
        "id": "b7942547",
        "papermill": {
          "duration": 0.256056,
          "end_time": "2021-11-07T17:22:13.413201",
          "exception": false,
          "start_time": "2021-11-07T17:22:13.157145",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2024-07-16T05:40:38.397722Z",
          "iopub.execute_input": "2024-07-16T05:40:38.398237Z",
          "iopub.status.idle": "2024-07-16T05:40:38.588227Z",
          "shell.execute_reply.started": "2024-07-16T05:40:38.398185Z",
          "shell.execute_reply": "2024-07-16T05:40:38.586949Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#This line code converts floating numericals into integer numeric\n",
        "#example: converting 1.0, 1.00, etc into integer 1\n",
        "#example: converting 0.0, 0.00, etc into integer 0\n",
        "\n",
        "df['tagging']=df['tagging'].astype(str).astype(int)"
      ],
      "metadata": {
        "id": "3d836434",
        "papermill": {
          "duration": 0.069784,
          "end_time": "2021-11-07T17:22:13.520968",
          "exception": false,
          "start_time": "2021-11-07T17:22:13.451184",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2024-07-16T05:40:38.589953Z",
          "iopub.execute_input": "2024-07-16T05:40:38.590445Z",
          "iopub.status.idle": "2024-07-16T05:40:38.617789Z",
          "shell.execute_reply.started": "2024-07-16T05:40:38.590399Z",
          "shell.execute_reply": "2024-07-16T05:40:38.616172Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "a16f8a83",
        "papermill": {
          "duration": 0.054797,
          "end_time": "2021-11-07T17:22:13.613912",
          "exception": false,
          "start_time": "2021-11-07T17:22:13.559115",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2024-07-16T05:40:38.619848Z",
          "iopub.execute_input": "2024-07-16T05:40:38.620265Z",
          "iopub.status.idle": "2024-07-16T05:40:38.640763Z",
          "shell.execute_reply.started": "2024-07-16T05:40:38.620229Z",
          "shell.execute_reply": "2024-07-16T05:40:38.639464Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "32e86fc9",
        "papermill": {
          "duration": 0.044831,
          "end_time": "2021-11-07T17:22:13.699238",
          "exception": false,
          "start_time": "2021-11-07T17:22:13.654407",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2024-07-16T05:40:38.642549Z",
          "iopub.execute_input": "2024-07-16T05:40:38.642934Z",
          "iopub.status.idle": "2024-07-16T05:40:38.655645Z",
          "shell.execute_reply.started": "2024-07-16T05:40:38.642893Z",
          "shell.execute_reply": "2024-07-16T05:40:38.654342Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#reseting the index of rows\n",
        "# Index gets unordered if we drop some rows in our dataset(Example while using dropna() function)\n",
        "df.reset_index(inplace = True, drop = True)"
      ],
      "metadata": {
        "id": "11b5bb3f",
        "papermill": {
          "duration": 0.04519,
          "end_time": "2021-11-07T17:22:13.783329",
          "exception": false,
          "start_time": "2021-11-07T17:22:13.738139",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2024-07-16T05:40:38.657422Z",
          "iopub.execute_input": "2024-07-16T05:40:38.657902Z",
          "iopub.status.idle": "2024-07-16T05:40:38.667454Z",
          "shell.execute_reply.started": "2024-07-16T05:40:38.657855Z",
          "shell.execute_reply": "2024-07-16T05:40:38.665986Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#this line of code will now be used to pply the functions on each sentences in the 'comments' column\n",
        "#This will take time as it will use the 'clean_text' function on all the sentences in our dataset\n",
        "#the .map() function applies the function at each sentences in the 'comments' column\n",
        "df['Processed_Comment'] = df['comments'].map(clean_text)"
      ],
      "metadata": {
        "id": "fd2a83a1",
        "papermill": {
          "duration": 14.564294,
          "end_time": "2021-11-07T17:22:28.387071",
          "exception": false,
          "start_time": "2021-11-07T17:22:13.822777",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2024-07-16T05:40:38.669025Z",
          "iopub.execute_input": "2024-07-16T05:40:38.66945Z",
          "iopub.status.idle": "2024-07-16T05:40:55.823844Z",
          "shell.execute_reply.started": "2024-07-16T05:40:38.669412Z",
          "shell.execute_reply": "2024-07-16T05:40:55.822483Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Splitting dataset for training and testing(80:20)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "#train_test_split is a funtion that splits dataset into two parts\n",
        "#80%(for training the model) and 20%(for testing the model)\n",
        "#This function returns 4 values\n",
        "# 1 'Processed_comment' for training\n",
        "# 2 'Processed_comment' for testing\n",
        "# 3 'tagging' for training\n",
        "# 3 'tagging' for testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['Processed_Comment'],\n",
        "                                                    df['tagging'],\n",
        "                                                    random_state=42,test_size=0.20)\n",
        "\n",
        "#random state is used to shuffle the dataset\n",
        "#test_size=0.20 means that 20% of the dataset is to be allocated for testing of the model"
      ],
      "metadata": {
        "id": "a54492db",
        "papermill": {
          "duration": 0.050042,
          "end_time": "2021-11-07T17:22:28.476927",
          "exception": false,
          "start_time": "2021-11-07T17:22:28.426885",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2024-07-16T05:40:55.825376Z",
          "iopub.execute_input": "2024-07-16T05:40:55.825762Z",
          "iopub.status.idle": "2024-07-16T05:40:55.83864Z",
          "shell.execute_reply.started": "2024-07-16T05:40:55.825727Z",
          "shell.execute_reply": "2024-07-16T05:40:55.837249Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating a bag of words from training data\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "count_vector = CountVectorizer()\n",
        "X_train = count_vector.fit_transform(X_train)\n",
        "X_test = count_vector.transform(X_test)\n",
        "\n",
        "#vectorizing means giving value to the words in the sentence according to a formula\n",
        "#This value tells us how much the word contributes in the sentence to be a cyberbulling comment\n",
        "#I guess this returns a table"
      ],
      "metadata": {
        "id": "de3193f1",
        "papermill": {
          "duration": 0.288811,
          "end_time": "2021-11-07T17:22:28.804992",
          "exception": false,
          "start_time": "2021-11-07T17:22:28.516181",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2024-07-16T05:40:55.840647Z",
          "iopub.execute_input": "2024-07-16T05:40:55.841183Z",
          "iopub.status.idle": "2024-07-16T05:40:56.067318Z",
          "shell.execute_reply.started": "2024-07-16T05:40:55.84113Z",
          "shell.execute_reply": "2024-07-16T05:40:56.066137Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#gives the number of columns in the vectorized table\n",
        "len(count_vector.vocabulary_)"
      ],
      "metadata": {
        "id": "4a1be2e5",
        "papermill": {
          "duration": 0.045125,
          "end_time": "2021-11-07T17:22:28.888514",
          "exception": false,
          "start_time": "2021-11-07T17:22:28.843389",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2024-07-16T05:40:56.068973Z",
          "iopub.execute_input": "2024-07-16T05:40:56.069497Z",
          "iopub.status.idle": "2024-07-16T05:40:56.078596Z",
          "shell.execute_reply.started": "2024-07-16T05:40:56.069449Z",
          "shell.execute_reply": "2024-07-16T05:40:56.077296Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Importing all the terms by which we get accuracy of our model\n",
        "#NOTE:we haven't trained our model yet\n",
        "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
        "#creates a confusion matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "data =[] #Empty list created to add the accuracy terms of each model that we are going to train"
      ],
      "metadata": {
        "id": "b16db899",
        "papermill": {
          "duration": 0.045796,
          "end_time": "2021-11-07T17:22:28.972907",
          "exception": false,
          "start_time": "2021-11-07T17:22:28.927111",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2024-07-16T05:40:56.080338Z",
          "iopub.execute_input": "2024-07-16T05:40:56.08082Z",
          "iopub.status.idle": "2024-07-16T05:40:56.090732Z",
          "shell.execute_reply.started": "2024-07-16T05:40:56.080772Z",
          "shell.execute_reply": "2024-07-16T05:40:56.08925Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing all the necessary models\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.datasets import load_iris\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load sample data\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Creating a dictionary of all the possible parameters of each ML model\n",
        "model_params = {\n",
        "    'LinearSVC': {\n",
        "        'model': LinearSVC(max_iter=1000000, random_state=42),\n",
        "        'params' : {\n",
        "            'C': [0.1, 1, 5, 10, 20],\n",
        "        }\n",
        "    },\n",
        "    'MultinomialNB': {\n",
        "        'model': MultinomialNB(),\n",
        "        'params' : {\n",
        "            'alpha': np.linspace(0.5, 1.5, 6),\n",
        "            'fit_prior': [True, False],\n",
        "        }\n",
        "    },\n",
        "    'LogisticRegression': {\n",
        "        'model': LogisticRegression(random_state=42, max_iter=1000000),\n",
        "        'params': {\n",
        "            'C': [1, 5, 10],\n",
        "            'solver': ['newton-cg', 'lbfgs', 'liblinear'],\n",
        "        }\n",
        "    },\n",
        "    'KNeighborsClassifier': {\n",
        "        'model': KNeighborsClassifier(),\n",
        "        'params' : {\n",
        "            'n_neighbors': [5, 9, 11, 23],\n",
        "            'weights': ['uniform', 'distance'],\n",
        "            'metric': ['euclidean', 'manhattan', 'minkowski'],\n",
        "        }\n",
        "    },\n",
        "    'RandomForestClassifier': {\n",
        "        'model': RandomForestClassifier(random_state=42),\n",
        "        'params' : {\n",
        "            'n_estimators': [10, 50, 100],\n",
        "            'criterion': ['gini', 'entropy'],\n",
        "            'max_depth': [None, 10, 20, 30],\n",
        "        }\n",
        "    },\n",
        "    'GradientBoostingClassifier': {\n",
        "        'model': GradientBoostingClassifier(random_state=42),\n",
        "        'params' : {\n",
        "            'n_estimators': [50, 100, 200],\n",
        "            'learning_rate': [0.01, 0.1, 0.2],\n",
        "            'max_depth': [3, 5, 7],\n",
        "        }\n",
        "    },\n",
        "    'AdaBoostClassifier': {\n",
        "        'model': AdaBoostClassifier(random_state=42),\n",
        "        'params' : {\n",
        "            'n_estimators': [50, 100, 200],\n",
        "            'learning_rate': [0.01, 0.1, 1],\n",
        "        }\n",
        "    },\n",
        "    'DecisionTreeClassifier': {\n",
        "        'model': DecisionTreeClassifier(random_state=42),\n",
        "        'params' : {\n",
        "            'criterion': ['gini', 'entropy'],\n",
        "            'max_depth': [None, 10, 20, 30],\n",
        "            'min_samples_split': [2, 5, 10],\n",
        "        }\n",
        "    },\n",
        "    'MLPClassifier': {\n",
        "        'model': MLPClassifier(random_state=42, max_iter=2000),\n",
        "        'params' : {\n",
        "            'hidden_layer_sizes': [(50, 50), (100,)],\n",
        "            'activation': ['tanh', 'relu'],\n",
        "            'solver': ['sgd', 'adam'],\n",
        "            'alpha': [0.0001, 0.001],\n",
        "            'learning_rate_init': [0.001, 0.01, 0.1]\n",
        "        }\n",
        "    },\n",
        "    'ExtraTreesClassifier': {\n",
        "        'model': ExtraTreesClassifier(random_state=42),\n",
        "        'params' : {\n",
        "            'n_estimators': [10, 50, 100],\n",
        "            'criterion': ['gini', 'entropy'],\n",
        "            'max_depth': [None, 10, 20, 30],\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "# Creating a list to store the best parameters\n",
        "scores = []\n",
        "\n",
        "for model_name, mp in model_params.items():\n",
        "    clf = GridSearchCV(mp['model'], mp['params'], cv=5, return_train_score=False)\n",
        "    clf.fit(X_train, y_train)\n",
        "    # storing the values in 'scores' list\n",
        "    scores.append({\n",
        "        'model': model_name,\n",
        "        'best_score': clf.best_score_,\n",
        "        'best_params': clf.best_params_\n",
        "    })\n",
        "\n",
        "# Creating a table of the best parameters\n",
        "df4 = pd.DataFrame(scores, columns=['model', 'best_score', 'best_params'])\n",
        "print(df4)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-16T06:02:05.088732Z",
          "iopub.execute_input": "2024-07-16T06:02:05.089188Z",
          "iopub.status.idle": "2024-07-16T06:04:07.451811Z",
          "shell.execute_reply.started": "2024-07-16T06:02:05.089152Z",
          "shell.execute_reply": "2024-07-16T06:04:07.450308Z"
        },
        "trusted": true,
        "id": "nMMOwGtWIrku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.metrics import accuracy_score, recall_score, f1_score, precision_score\n",
        "\n",
        "# Creating a dictionary of all the possible parameters of each ML model\n",
        "model_params = {\n",
        "    'LinearSVC': {\n",
        "        'model': LinearSVC(max_iter=1000000, random_state=42),\n",
        "        'params' : {\n",
        "            'C': [0.1, 1, 5, 10, 20],\n",
        "        }\n",
        "    },\n",
        "    'MultinomialNB': {\n",
        "        'model': MultinomialNB(),\n",
        "        'params' : {\n",
        "            'alpha': np.linspace(0.5, 1.5, 6),\n",
        "            'fit_prior': [True, False],\n",
        "        }\n",
        "    },\n",
        "    'LogisticRegression': {\n",
        "        'model': LogisticRegression(random_state=42, max_iter=1000000),\n",
        "        'params': {\n",
        "            'C': [1, 5, 10],\n",
        "            'solver': ['newton-cg', 'lbfgs', 'liblinear'],\n",
        "        }\n",
        "    },\n",
        "    'KNeighborsClassifier': {\n",
        "        'model': KNeighborsClassifier(),\n",
        "        'params' : {\n",
        "            'n_neighbors': [5, 9, 11, 23],\n",
        "            'weights': ['uniform', 'distance'],\n",
        "            'metric': ['euclidean', 'manhattan', 'minkowski'],\n",
        "        }\n",
        "    },\n",
        "    'RandomForestClassifier': {\n",
        "        'model': RandomForestClassifier(random_state=42),\n",
        "        'params' : {\n",
        "            'n_estimators': [10, 50, 100],\n",
        "            'criterion': ['gini', 'entropy'],\n",
        "            'max_depth': [None, 10, 20, 30],\n",
        "        }\n",
        "    },\n",
        "    'GradientBoostingClassifier': {\n",
        "        'model': GradientBoostingClassifier(random_state=42),\n",
        "        'params' : {\n",
        "            'n_estimators': [50, 100, 200],\n",
        "            'learning_rate': [0.01, 0.1, 0.2],\n",
        "            'max_depth': [3, 5, 7],\n",
        "        }\n",
        "    },\n",
        "    'AdaBoostClassifier': {\n",
        "        'model': AdaBoostClassifier(random_state=42),\n",
        "        'params' : {\n",
        "            'n_estimators': [50, 100, 200],\n",
        "            'learning_rate': [0.01, 0.1, 1],\n",
        "        }\n",
        "    },\n",
        "    'DecisionTreeClassifier': {\n",
        "        'model': DecisionTreeClassifier(random_state=42),\n",
        "        'params' : {\n",
        "            'criterion': ['gini', 'entropy'],\n",
        "            'max_depth': [None, 10, 20, 30],\n",
        "            'min_samples_split': [2, 5, 10],\n",
        "        }\n",
        "    },\n",
        "    'MLPClassifier': {\n",
        "        'model': MLPClassifier(random_state=42, max_iter=2000),\n",
        "        'params' : {\n",
        "            'hidden_layer_sizes': [(50, 50), (100,)],\n",
        "            'activation': ['tanh', 'relu'],\n",
        "            'solver': ['sgd', 'adam'],\n",
        "            'alpha': [0.0001, 0.001],\n",
        "            'learning_rate_init': [0.001, 0.01, 0.1]\n",
        "        }\n",
        "    },\n",
        "    'ExtraTreesClassifier': {\n",
        "        'model': ExtraTreesClassifier(random_state=42),\n",
        "        'params' : {\n",
        "            'n_estimators': [10, 50, 100],\n",
        "            'criterion': ['gini', 'entropy'],\n",
        "            'max_depth': [None, 10, 20, 30],\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "# Creating a list to store the best parameters and their evaluation scores\n",
        "scores = []\n",
        "data = []\n",
        "\n",
        "for model_name, mp in model_params.items():\n",
        "    clf = GridSearchCV(mp['model'], mp['params'], cv=5, return_train_score=False)\n",
        "    clf.fit(X_train, y_train)\n",
        "\n",
        "    # Best estimator from GridSearchCV\n",
        "    best_model = clf.best_estimator_\n",
        "\n",
        "    # Predictions for training and testing datasets\n",
        "    predictions_train = best_model.predict(X_train)\n",
        "    predictions_test = best_model.predict(X_test)\n",
        "\n",
        "    # Storing the best parameters and scores\n",
        "    scores.append({\n",
        "        'model': model_name,\n",
        "        'best_score': clf.best_score_,\n",
        "        'best_params': clf.best_params_\n",
        "    })\n",
        "\n",
        "    # Creating a temp list to store the accuracy terms\n",
        "    temp = [model_name]\n",
        "    temp.append(accuracy_score(predictions_train, y_train))  # accuracy_score for training data\n",
        "    temp.append(recall_score(predictions_train, y_train, average='macro'))    # recall_score for training data\n",
        "    temp.append(f1_score(predictions_train, y_train, average='macro'))        # f1_score for training data\n",
        "    temp.append(precision_score(predictions_train, y_train, average='macro')) # precision_score for training data\n",
        "    temp.append(accuracy_score(predictions_test, y_test))    # accuracy_score for testing data\n",
        "    temp.append(recall_score(predictions_test, y_test, average='macro'))      # recall_score for testing data\n",
        "    temp.append(f1_score(predictions_test, y_test, average='macro'))          # f1_score for testing data\n",
        "    temp.append(precision_score(predictions_test, y_test, average='macro'))   # precision_score for testing data\n",
        "\n",
        "    # Storing all the accuracy terms in 'data' list\n",
        "    data.append(temp)\n",
        "\n",
        "# Creating a DataFrame for best parameters and their evaluation scores\n",
        "df4 = pd.DataFrame(scores, columns=['model', 'best_score', 'best_params'])\n",
        "print(df4)\n",
        "\n",
        "# Creating a DataFrame for accuracy terms\n",
        "columns = ['Model', 'Train Accuracy', 'Train Recall', 'Train F1', 'Train Precision', 'Test Accuracy', 'Test Recall', 'Test F1', 'Test Precision']\n",
        "df5 = pd.DataFrame(data, columns=columns)\n",
        "print(df5)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-16T06:10:38.371902Z",
          "iopub.execute_input": "2024-07-16T06:10:38.372451Z",
          "iopub.status.idle": "2024-07-16T06:12:41.280606Z",
          "shell.execute_reply.started": "2024-07-16T06:10:38.372408Z",
          "shell.execute_reply": "2024-07-16T06:12:41.279184Z"
        },
        "trusted": true,
        "id": "SjP4NuApIrkv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summary"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.077551,
          "end_time": "2021-11-07T17:27:40.690885",
          "exception": false,
          "start_time": "2021-11-07T17:27:40.613334",
          "status": "completed"
        },
        "tags": [],
        "id": "c29e51aa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating a table of all accuracy terms of each trained ML models\n",
        "result = pd.DataFrame(data, columns = ['Algorithm','Accuracy Score : Train', 'Recall Score : Train','F1-Score :Train','Precision Score :Train','Accuracy Score : Test', 'Recall Score : Test','F1-Score : Test','Precision Score : Test'])\n",
        "result.reset_index(drop=True, inplace=True)"
      ],
      "metadata": {
        "id": "c61be225",
        "papermill": {
          "duration": 0.083984,
          "end_time": "2021-11-07T17:27:40.850074",
          "exception": false,
          "start_time": "2021-11-07T17:27:40.76609",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2024-07-16T06:14:35.832367Z",
          "iopub.execute_input": "2024-07-16T06:14:35.832948Z",
          "iopub.status.idle": "2024-07-16T06:14:35.842391Z",
          "shell.execute_reply.started": "2024-07-16T06:14:35.832902Z",
          "shell.execute_reply": "2024-07-16T06:14:35.840748Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result"
      ],
      "metadata": {
        "id": "d7b3ec9d",
        "papermill": {
          "duration": 0.077904,
          "end_time": "2021-11-07T17:27:40.999044",
          "exception": false,
          "start_time": "2021-11-07T17:27:40.92114",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2024-07-16T06:14:43.493578Z",
          "iopub.execute_input": "2024-07-16T06:14:43.494042Z",
          "iopub.status.idle": "2024-07-16T06:14:43.514171Z",
          "shell.execute_reply.started": "2024-07-16T06:14:43.494003Z",
          "shell.execute_reply": "2024-07-16T06:14:43.512827Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Extracting data for plotting\n",
        "algorithms = result['Algorithm']\n",
        "train_accuracy = result['Accuracy Score : Train']\n",
        "test_accuracy = result['Accuracy Score : Test']\n",
        "train_recall = result['Recall Score : Train']\n",
        "test_recall = result['Recall Score : Test']\n",
        "train_f1 = result['F1-Score :Train']\n",
        "test_f1 = result['F1-Score : Test']\n",
        "train_precision = result['Precision Score :Train']\n",
        "test_precision = result['Precision Score : Test']\n",
        "\n",
        "# Setting up the figure and axes\n",
        "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(16, 12))\n",
        "plt.subplots_adjust(hspace=0.4)\n",
        "\n",
        "# Defining color palettes\n",
        "train_color = 'skyblue'\n",
        "test_color = 'orange'\n",
        "\n",
        "# Plotting Accuracy Scores\n",
        "sns.barplot(x=algorithms, y=train_accuracy, ax=axes[0, 0], color=train_color, label='Train Accuracy')\n",
        "sns.barplot(x=algorithms, y=test_accuracy, ax=axes[0, 0], color=test_color, label='Test Accuracy')\n",
        "axes[0, 0].set_title('Accuracy Scores')\n",
        "axes[0, 0].set_ylabel('Score')\n",
        "axes[0, 0].legend()\n",
        "\n",
        "# Plotting Recall Scores\n",
        "sns.barplot(x=algorithms, y=train_recall, ax=axes[0, 1], color=train_color, label='Train Recall')\n",
        "sns.barplot(x=algorithms, y=test_recall, ax=axes[0, 1], color=test_color, label='Test Recall')\n",
        "axes[0, 1].set_title('Recall Scores')\n",
        "axes[0, 1].set_ylabel('Score')\n",
        "axes[0, 1].legend()\n",
        "\n",
        "# Plotting F1-Scores\n",
        "sns.barplot(x=algorithms, y=train_f1, ax=axes[1, 0], color=train_color, label='Train F1-Score')\n",
        "sns.barplot(x=algorithms, y=test_f1, ax=axes[1, 0], color=test_color, label='Test F1-Score')\n",
        "axes[1, 0].set_title('F1-Scores')\n",
        "axes[1, 0].set_ylabel('Score')\n",
        "axes[1, 0].legend()\n",
        "\n",
        "# Plotting Precision Scores\n",
        "sns.barplot(x=algorithms, y=train_precision, ax=axes[1, 1], color=train_color, label='Train Precision')\n",
        "sns.barplot(x=algorithms, y=test_precision, ax=axes[1, 1], color=test_color, label='Test Precision')\n",
        "axes[1, 1].set_title('Precision Scores')\n",
        "axes[1, 1].set_ylabel('Score')\n",
        "axes[1, 1].legend()\n",
        "\n",
        "# Displaying the plot\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-16T06:43:58.875774Z",
          "iopub.execute_input": "2024-07-16T06:43:58.876888Z",
          "iopub.status.idle": "2024-07-16T06:44:00.208429Z",
          "shell.execute_reply.started": "2024-07-16T06:43:58.876843Z",
          "shell.execute_reply": "2024-07-16T06:44:00.20724Z"
        },
        "trusted": true,
        "id": "HpO2Ek9-Irkw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Saving models in .pkl file"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.043222,
          "end_time": "2021-11-07T17:27:41.086921",
          "exception": false,
          "start_time": "2021-11-07T17:27:41.043699",
          "status": "completed"
        },
        "tags": [],
        "id": "57e2e9c5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import LinearSVC, SVC\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "# Define all the models\n",
        "clfs = {\n",
        "    \"Random Forest\": RandomForestClassifier(random_state=42),\n",
        "    \"Gradient Boosting\": GradientBoostingClassifier(random_state=42),\n",
        "    \"AdaBoost\": AdaBoostClassifier(random_state=42),\n",
        "    \"LightGBM\": LGBMClassifier(random_state=42),\n",
        "    \"XGBoost\": XGBClassifier(eval_metric=\"mlogloss\", random_state=42),\n",
        "    \"Decision Tree\": DecisionTreeClassifier(random_state=42),\n",
        "    \"Support Vector Machine\": SVC(random_state=42),\n",
        "    \"Naive Bayes\": MultinomialNB(),\n",
        "    \"Multilayer Perceptron\": MLPClassifier(random_state=42, max_iter=1000),\n",
        "    \"KNeighborsClassifier\": KNeighborsClassifier()\n",
        "}\n",
        "\n",
        "# Assuming you have the training and testing data\n",
        "# X_train, X_test, y_train, y_test\n",
        "\n",
        "# List to store the accuracy of each model\n",
        "accuracies = []\n",
        "\n",
        "# Train each model and save it to a .pkl file\n",
        "for name, clf in clfs.items():\n",
        "    # Train the model\n",
        "    clf.fit(X_train, y_train)\n",
        "\n",
        "    # Save the trained model to a .pkl file\n",
        "    with open(f'{name}.pkl', 'wb') as file:\n",
        "        pickle.dump(clf, file)\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = clf.score(X_test, y_test)\n",
        "    accuracies.append((name, accuracy))\n",
        "\n",
        "# Print the accuracy of the models\n",
        "for name, accuracy in accuracies:\n",
        "    print(f'Model: {name}, Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-16T06:21:31.628038Z",
          "iopub.execute_input": "2024-07-16T06:21:31.62853Z",
          "iopub.status.idle": "2024-07-16T06:21:33.677103Z",
          "shell.execute_reply.started": "2024-07-16T06:21:31.628489Z",
          "shell.execute_reply": "2024-07-16T06:21:33.675801Z"
        },
        "trusted": true,
        "id": "KV_0mHdeIrkx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "papermill": {
          "duration": 0.043308,
          "end_time": "2021-11-07T17:27:41.745733",
          "exception": false,
          "start_time": "2021-11-07T17:27:41.702425",
          "status": "completed"
        },
        "tags": [],
        "id": "ed19b67e"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}